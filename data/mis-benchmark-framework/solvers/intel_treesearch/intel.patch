diff --git a/demo.py b/demo.py
index 258d782..c6c1410 100644
--- a/demo.py
+++ b/demo.py
@@ -1,44 +1,78 @@
 from __future__ import division
 from __future__ import print_function
 
+### Begin argument parsing
+import argparse
+
+parser = argparse.ArgumentParser(description="Intel-based tree search.")
+parser.add_argument("input", type=str, action="store", help="Directory containing input graphs to be solved")
+parser.add_argument("output", type=str, action="store",  help="Folder in which the output will be stored")
+parser.add_argument("pretrained_weights", type=str, action="store", help="Pre-trained weights to be used for solving (folder containg checkpoints)")
+
+parser.add_argument("--time_limit", type=int, nargs="?", action="store", default=600, help="Time limit in seconds")
+parser.add_argument("--cuda_device", type=int, nargs="*", action="store", default=0, help="Which cuda device should be used")
+parser.add_argument("--self_loops", action="store_true", default=False, help="Enable self loops addition (in input data) for GCN-based model.")
+parser.add_argument("--reduction", action="store_true", default=False, help="Enable reduction of graph (kernelization).")
+parser.add_argument("--local_search", action="store_true", default=False, help="Enable local search if time left.")
+parser.add_argument("--model_prob_maps", type=int, action="store", default=32, help="Number of probability maps.")
+
+args = parser.parse_args()
+
+prob_maps = args.model_prob_maps
+
+### End argument parsing
+
 import sys
 import os
 sys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )
 # add the libary path for graph reduction and local search
-# sys.path.append( '%s/kernel' % os.path.dirname(os.path.realpath(__file__)) )
+
+if args.reduction or args.local_search:
+    sys.path.append( '%s/kernel' % os.path.dirname(os.path.realpath(__file__)) )
 
 import time
 import scipy.io as sio
 import numpy as np
 import scipy.sparse as sp
-import Queue
 from copy import deepcopy
 
 # import the libary for graph reduction and local search
-# from reduce_lib import reducelib
+if args.reduction or args.local_search:
+    from reduce_lib import reducelib
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 from utils import *
 from models import GCN_DEEP_DIVER
 
+import statistics
+
 N_bd = 32
 
 # Settings
-flags = tf.app.flags
+flags = tf.flags
 FLAGS = flags.FLAGS
 flags.DEFINE_string('model', 'gcn_cheby', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'
 flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')
 flags.DEFINE_integer('epochs', 201, 'Number of epochs to train.')
 flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')
-flags.DEFINE_integer('diver_num', 32, 'Number of outputs.')
+flags.DEFINE_integer('diver_num', prob_maps, 'Number of outputs.')
 flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probaNUmbility).')
 flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')
 flags.DEFINE_integer('early_stopping', 1000, 'Tolerance for early stopping (# of epochs).')
 flags.DEFINE_integer('max_degree', 1, 'Maximum Chebyshev polynomial degree.')
 flags.DEFINE_integer('num_layer', 20, 'number of layers.')
 
+# we need to define our argparse argument here aswell, otherwise tf.flags throws an exception
+flags.DEFINE_string("time_limit", "", "")
+flags.DEFINE_string("cuda_device", "", "")
+flags.DEFINE_boolean("self_loops", False, "")
+flags.DEFINE_boolean("reduction", False, "")
+flags.DEFINE_boolean("local_search", False, "")
+flags.DEFINE_string("model_prob_maps", "", "")
+
 # test data path
-data_path = "./data"
+data_path = args.input
 val_mat_names = os.listdir(data_path)
 
 # Some preprocessing
@@ -60,7 +94,7 @@ placeholders = {
 model = model_func(placeholders, input_dim=N_bd, logging=True)
 
 # use gpu 0
-os.environ['CUDA_VISIBLE_DEVICES']=str(0)
+os.environ['CUDA_VISIBLE_DEVICES']=str(args.cuda_device)
 
 # Initialize session
 config = tf.ConfigProto()
@@ -126,8 +160,10 @@ def reduce_graph(adj, nIS_vec_local):
     remain_vec = (nIS_vec_local == -1)
 
     # reduce graph
-    # reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = api.reduce_graph(adj)
-    reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = fake_reduce_graph(adj)
+    if args.reduction:
+        reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = api.reduce_graph(adj)
+    else:
+        reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = fake_reduce_graph(adj)
     nIS_vec_sub = reduced_node.copy()
     nIS_vec_sub_tmp = reduced_node.copy()
     nIS_vec_sub[nIS_vec_sub_tmp == 0] = 1
@@ -146,13 +182,17 @@ def reduce_graph(adj, nIS_vec_local):
         if np.sum(remain_vec_tmp) == 0:
             # get a solution
             res_ct += 1
-            # nIS_vec_local = api.local_search(adj_0, nIS_vec_local)
-            nIS_vec_local = fake_local_search(adj_0, nIS_vec_local)
+            if args.local_search:
+                nIS_vec_local = api.local_search(adj_0, nIS_vec_local)
+            else:
+                nIS_vec_local = fake_local_search(adj_0, nIS_vec_local)
             if np.sum(nIS_vec_local) > best_IS_num:
                 best_IS_num = np.sum(nIS_vec_local)
                 best_IS_vec = deepcopy(nIS_vec_local)
-                sio.savemat('./res_%04d/%s' % (
+                sio.savemat(args.output + '/res_%04d/%s' % (
                     time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+                statistics.collector.current_collector.collect_result(np.flatnonzero(best_IS_vec))
+
             print("ID: %03d" % id, "QItem: %03d" % q_ct, "Res#: %03d" % res_ct,
                   "Current: %d" % (np.sum(nIS_vec_local)), "Best: %d" % best_IS_num, "Reduction")
             return True
@@ -169,25 +209,42 @@ def reduce_graph(adj, nIS_vec_local):
 saver=tf.train.Saver(max_to_keep=1000)
 sess.run(tf.global_variables_initializer())
 
-ckpt=tf.train.get_checkpoint_state("./model")
+ckpt=tf.train.get_checkpoint_state(args.pretrained_weights)
 print('loaded '+ckpt.model_checkpoint_path)
 saver.restore(sess,ckpt.model_checkpoint_path)
 
 noout = FLAGS.diver_num # number of outputs
-time_limit = 600  # time limit for searching
+time_limit = args.time_limit # time limit for searching
 
-if not os.path.isdir("./res_%04d"%time_limit):
-    os.makedirs("./res_%04d"%time_limit)
+if not os.path.isdir(args.output + "/res_%04d"%time_limit):
+    os.makedirs(args.output + "/res_%04d"%time_limit)
 
 # for graph reduction and local search
-# api = reducelib()
+if args.reduction or args.local_search:
+    api = reducelib()
+
+if args.self_loops:
+    import scipy
 
 for id in range(len(val_mat_names)):
+    stat_collector = statistics.collector.new_collector(val_mat_names[id])
+    stat_collector.start_timer()
+    stat_collector.start_process_timer()
     best_IS_num = -1
     mat_contents = sio.loadmat(data_path + '/' + val_mat_names[id])
     adj_0 = mat_contents['adj']
-    # yy = mat_contents['indset_label']
-    # opt_num = np.sum(yy[:,0])
+
+    if args.self_loops:
+        identity = scipy.sparse.identity(adj_0.shape[0], dtype=adj_0.dtype, format=adj_0.format)
+        adj_0 = adj_0 + identity
+
+    labels_given = False
+    if 'indset_label' in mat_contents.keys():
+        yy = mat_contents['indset_label']
+        opt_num = np.sum(yy[:,0])
+        labels_given = True
+        print("Labels were given, terminating if optimal MIS found", file=sys.stderr)
+
     # edges_0 = sp.find(adj_0) # for isis version 1
     edges_0 = findNodeEdges(adj_0)
     nn = adj_0.shape[0]
@@ -199,8 +256,8 @@ for id in range(len(val_mat_names)):
     start_time = time.time()
     while time.time()-start_time < time_limit:
 
-        # if best_IS_num == opt_num:
-        #     break
+        if labels_given and best_IS_num == opt_num:
+            break
 
         if len(bsf_q) == 0:
             if reduce_graph(adj_0, -np.ones(nn)):
@@ -225,9 +282,15 @@ for id in range(len(val_mat_names)):
 
             _, z_out = evaluate(features, support, placeholders)
 
+            stat_collector.add_iteration()
+
             for out_id in range(noout):
-                # if best_IS_num == opt_num:
-                #     break
+
+                if time.time()-start_time > time_limit:
+                    break
+
+                if labels_given and best_IS_num == opt_num:
+                    break
 
                 nIS_vec = deepcopy(q_item[1])
                 nIS_Prob_sub_t = z_out[:, 2 * out_id + 1]
@@ -242,6 +305,9 @@ for id in range(len(val_mat_names)):
                 # tt = time.time()
                 nIS_vec_tmp = deepcopy(nIS_vec)
                 for cid in range(nn):
+                    if time.time()-start_time > time_limit:
+                        break
+                    
                     cn = cns_sorted[cid]
                     # check graph
                     if isis_v2(edges_0, nIS_vec_tmp, cn):
@@ -260,13 +326,16 @@ for id in range(len(val_mat_names)):
                 if np.sum(remain_vec_tmp) == 0:
                     # get a solution
                     res_ct += 1
-                    # nIS_vec = api.local_search(adj_0, nIS_vec)
-                    nIS_vec = fake_local_search(adj_0, nIS_vec)
+                    if args.local_search:
+                        nIS_vec = api.local_search(adj_0, nIS_vec)
+                    else:
+                        nIS_vec = fake_local_search(adj_0, nIS_vec)
                     if np.sum(nIS_vec) > best_IS_num:
                         best_IS_num = np.sum(nIS_vec)
                         best_IS_vec = deepcopy(nIS_vec)
-                        sio.savemat('./res_%04d/%s' % (
+                        sio.savemat(args.output + '/res_%04d/%s' % (
                         time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+                        statistics.collector.current_collector.collect_result(np.flatnonzero(best_IS_vec))
                     print("ID: %03d" % id, "QItem: %03d" % q_ct, "Res#: %03d" % res_ct,
                           "Current: %d" % (np.sum(nIS_vec)), "Best: %d" % best_IS_num, "Network")
                     continue
@@ -280,5 +349,10 @@ for id in range(len(val_mat_names)):
             nIS_vec = deepcopy(q_item[1])
             if reduce_graph(adj, nIS_vec):
                 continue
+    try:
+        sio.savemat(args.output + '/res_%04d/%s' % (time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+    except Exception as e:
+        print("Error while saving matrix")
+        print(e)
 
-    sio.savemat('./res_%04d/%s' % (time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+statistics.collector.finalize(args.output + "/results.json")
diff --git a/demo_parallel.py b/demo_parallel.py
index 95b5416..ed03542 100644
--- a/demo_parallel.py
+++ b/demo_parallel.py
@@ -1,28 +1,56 @@
 from __future__ import division
 from __future__ import print_function
 
+### Begin argument parsing
+import argparse
+
+parser = argparse.ArgumentParser(description="Intel-based tree search.")
+parser.add_argument("input", type=str, action="store", help="Directory containing input graphs to be solved")
+parser.add_argument("output", type=str, action="store",  help="Folder in which the output will be stored")
+parser.add_argument("pretrained_weights", type=str, action="store", help="Pre-trained weights to be used for solving (folder containg checkpoints)")
+
+parser.add_argument("--time_limit", type=int, nargs="?", action="store", default=600, help="Time limit in seconds")
+parser.add_argument("--cuda_device", type=int, nargs="*", action="store", default=0, help="Which cuda device should be used")
+parser.add_argument("--self_loops", action="store_true", default=False, help="Enable self loops addition (in input data) for GCN-based model.")
+parser.add_argument("--reduction", action="store_true", default=False, help="Enable reduction of graph (kernelization).")
+parser.add_argument("--local_search", action="store_true", default=False, help="Enable local search if time left.")
+parser.add_argument("--model_prob_maps", type=int, action="store", default=32, help="Number of probability maps.")
+parser.add_argument("--num_threads", type=int, action="store", default=16, help="Number of threads to use.")
+
+args = parser.parse_args()
+
+prob_maps = args.model_prob_maps
+
+### End argument parsing
 import sys
 import os
 sys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )
 # add the libary path for graph reduction and local search
-# sys.path.append( '%s/kernel' % os.path.dirname(os.path.realpath(__file__)) )
+if args.reduction or args.local_search:
+    sys.path.append( '%s/kernel' % os.path.dirname(os.path.realpath(__file__)) )
 
 import time
 import scipy.io as sio
 import numpy as np
 import scipy.sparse as sp
-import Queue
 import multiprocessing as mp
 from multiprocessing import Manager, Value, Lock
 from copy import deepcopy
+import pickle
+import tempfile
+import shutil
+from pathlib import Path
 
 # import the libary for graph reduction and local search
-# from reduce_lib import reducelib
+if args.reduction or args.local_search:
+    from reduce_lib import reducelib
 
 from utils import *
 
+import statistics
+from functools import reduce
 # test data path
-data_path = "./data"
+data_path = args.input
 val_mat_names = os.listdir(data_path)
 
 # Define model evaluation function
@@ -46,7 +74,7 @@ def isis(edges, nIS_vec_local):
     tmp = (nIS_vec_local==1)
     return np.sum(tmp[edges[0]]*tmp[edges[1]]) > 0
 
-def add_rnd_q(cns, nIS_vec_local, pnum, lock):
+def add_rnd_q(cns, nIS_vec_local, pnum, lock, stat_collector):
     global adj_0
 
     nIS_vec_local[cns] = 1
@@ -56,7 +84,7 @@ def add_rnd_q(cns, nIS_vec_local, pnum, lock):
     adj = adj_0
     adj = adj[remain_vec_tmp, :]
     adj = adj[:, remain_vec_tmp]
-    if reduce_graph(adj, nIS_vec_local, pnum, lock):
+    if reduce_graph(adj, nIS_vec_local, pnum, lock, stat_collector):
         return True
     return False
 
@@ -71,7 +99,7 @@ def fake_reduce_graph(adj):
 def fake_local_search(adj, nIS_vec):
     return nIS_vec.astype(int)
 
-def reduce_graph(adj, nIS_vec_local, pnum, lock):
+def reduce_graph(adj, nIS_vec_local, pnum, lock, stat_collector):
     global best_IS_num
     global best_IS_vec
     global bsf_q
@@ -80,12 +108,15 @@ def reduce_graph(adj, nIS_vec_local, pnum, lock):
     global id
     global out_id
     global res_ct
+    global sc
 
     remain_vec = (nIS_vec_local == -1)
 
     # reduce graph
-    # reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = api.reduce_graph(adj)
-    reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = fake_reduce_graph(adj)
+    if args.reduction:
+        reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = api.reduce_graph(adj)
+    else:
+        reduced_node, reduced_adj, mapping, reverse_mapping, crt_is_size = fake_reduce_graph(adj)
     nIS_vec_sub = reduced_node.copy()
     nIS_vec_sub_tmp = reduced_node.copy()
     nIS_vec_sub[nIS_vec_sub_tmp == 0] = 1
@@ -106,14 +137,17 @@ def reduce_graph(adj, nIS_vec_local, pnum, lock):
             with lock:
                 res_ct.value += 1
                 local_res_ct = res_ct.value
-            # nIS_vec_local = api.local_search(adj_0, nIS_vec_local)
-            nIS_vec_local = fake_local_search(adj_0, nIS_vec_local)
+            if args.local_search:
+                nIS_vec_local = api.local_search(adj_0, nIS_vec_local)
+            else:
+                nIS_vec_local = fake_local_search(adj_0, nIS_vec_local)
             with lock:
                 if np.sum(nIS_vec_local) > best_IS_num.value:
                     best_IS_num.value = np.sum(nIS_vec_local)
                     best_IS_vec = deepcopy(nIS_vec_local)
-                    sio.savemat('./res_%04d/%s' % (
+                    sio.savemat(args.output + '/res_%04d/%s' % (
                         time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+                    stat_collector.collect_result(np.flatnonzero(best_IS_vec))
             print("PID: %02d" % pnum, "ID: %03d" % id, "QItem: %03d" % q_ct.value, "Res#: %03d" % local_res_ct,
                     "Current: %d" % (np.sum(nIS_vec_local)), "Best: %d" % best_IS_num.value, "Reduction")
             return True
@@ -128,9 +162,12 @@ def reduce_graph(adj, nIS_vec_local, pnum, lock):
 
     return False
 
-def MPSearch(pnum, lock):
+def MPSearch(pnum, lock, stat_collector, pickle_path, labels_given):
+    stat_collector.start_process_timer()
+
+    import tensorflow.compat.v1 as tf
+    tf.disable_v2_behavior()
 
-    import tensorflow as tf
     from models import GCN_DEEP_DIVER
 
     global best_IS_num  #
@@ -148,19 +185,28 @@ def MPSearch(pnum, lock):
     global N_bd
 
     # Settings
-    flags = tf.app.flags
+    flags = tf.flags
     FLAGS = flags.FLAGS
     flags.DEFINE_string('model', 'gcn_cheby', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'
     flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')
     flags.DEFINE_integer('epochs', 201, 'Number of epochs to train.')
     flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')
-    flags.DEFINE_integer('diver_num', 32, 'Number of outputs.')
+    flags.DEFINE_integer('diver_num', prob_maps, 'Number of outputs.')
     flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probaNUmbility).')
     flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')
     flags.DEFINE_integer('early_stopping', 1000, 'Tolerance for early stopping (# of epochs).')
     flags.DEFINE_integer('max_degree', 1, 'Maximum Chebyshev polynomial degree.')
     flags.DEFINE_integer('num_layer', 20, 'number of layers.')
 
+    # we need to define our argparse argument here aswell, otherwise tf.flags throws an exception
+    flags.DEFINE_string("time_limit", "", "")
+    flags.DEFINE_string("cuda_device", "", "")
+    flags.DEFINE_boolean("self_loops", False, "")
+    flags.DEFINE_boolean("reduction", False, "")
+    flags.DEFINE_boolean("local_search", False, "")
+    flags.DEFINE_string("model_prob_maps", "", "")
+    flags.DEFINE_string("num_threads", "", "")
+
     # Some preprocessing
 
     num_supports = 1 + FLAGS.max_degree
@@ -182,7 +228,7 @@ def MPSearch(pnum, lock):
     # os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')
     # os.environ['CUDA_VISIBLE_DEVICES']=str(np.argmax([int(x.split()[2]) for x in open('tmp','r').readlines()]))
     # os.system('rm tmp')
-    os.environ['CUDA_VISIBLE_DEVICES'] = str(0)
+    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.cuda_device)
 
     # Initialize session
     config = tf.ConfigProto()
@@ -194,19 +240,17 @@ def MPSearch(pnum, lock):
     saver = tf.train.Saver(max_to_keep=1000)
     sess.run(tf.global_variables_initializer())
 
-    ckpt = tf.train.get_checkpoint_state("./model")
+    ckpt = tf.train.get_checkpoint_state(args.pretrained_weights)
     print('%02d loaded' % pnum + ckpt.model_checkpoint_path)
     saver.restore(sess, ckpt.model_checkpoint_path)
 
     noout = FLAGS.diver_num  # number of outputs
-
     while time.time()-start_time < time_limit:
-
-        # if best_IS_num.value == opt_num:
-        #     break
+        if labels_given and best_IS_num.value == opt_num:
+             break
 
         if len(bsf_q) == 0:
-            if reduce_graph(adj_0, -np.ones(nn), pnum, lock):
+            if reduce_graph(adj_0, -np.ones(nn), pnum, lock, stat_collector):
                 break
 
         with lock:
@@ -232,9 +276,11 @@ def MPSearch(pnum, lock):
 
             _, z_out = evaluate(sess, model, features, support, placeholders)
 
+            stat_collector.add_iteration()
+
             out_id = np.random.randint(noout)
-            # if best_IS_num.value == opt_num:
-            #     break
+            if labels_given and best_IS_num.value == opt_num:
+                break
 
             nIS_vec = deepcopy(q_item[1])
             nIS_Prob_sub_t = z_out[:, 2 * out_id + 1]
@@ -255,7 +301,7 @@ def MPSearch(pnum, lock):
                 nIS_vec_tmp[cn] = 1
                 # check graph
                 if np.random.random_sample() > 0.7:
-                    add_rnd_q(cns_sorted[:(cid + 1)], deepcopy(nIS_vec), pnum, lock)
+                    add_rnd_q(cns_sorted[:(cid + 1)], deepcopy(nIS_vec), pnum, lock, stat_collector)
 
             # print("time=", "{:.5f}".format((time.time() - tt)))
 
@@ -264,19 +310,23 @@ def MPSearch(pnum, lock):
             tmp = sp.find(adj_0[cns, :] == 1)
             nIS_vec[tmp[1]] = 0
             remain_vec_tmp = (nIS_vec == -1)
+
             if np.sum(remain_vec_tmp) == 0:
                 # get a solution
                 with lock:
                     res_ct.value += 1
                     local_res_ct = res_ct.value
-                    # nIS_vec = api.local_search(adj_0, nIS_vec)
-                    nIS_vec = fake_local_search(adj_0, nIS_vec)
+                    if args.local_search:
+                        nIS_vec = api.local_search(adj_0, nIS_vec)
+                    else:
+                        nIS_vec = fake_local_search(adj_0, nIS_vec)
                 with lock:
                     if np.sum(nIS_vec) > best_IS_num.value:
                         best_IS_num.value = np.sum(nIS_vec)
                         best_IS_vec = deepcopy(nIS_vec)
-                        sio.savemat('./res_%04d/%s' % (
+                        sio.savemat(args.output + '/res_%04d/%s' % (
                         time_limit, val_mat_names[id]), {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+                        stat_collector.collect_result(np.flatnonzero(best_IS_vec))
                 print("PID: %02d" % pnum, "ID: %03d" % id, "QItem: %03d" % q_ct.value, "Res#: %03d" % local_res_ct,
                         "Current: %d" % (np.sum(nIS_vec)), "Best: %d" % best_IS_num.value, "Network")
                 continue
@@ -284,20 +334,30 @@ def MPSearch(pnum, lock):
             adj = adj[remain_vec_tmp, :]
             adj = adj[:, remain_vec_tmp]
 
-            if reduce_graph(adj, nIS_vec, pnum, lock):
+            if reduce_graph(adj, nIS_vec, pnum, lock, stat_collector):
                 continue
         else:
             nIS_vec = deepcopy(q_item[1])
-            if reduce_graph(adj, nIS_vec, pnum, lock):
+            if reduce_graph(adj, nIS_vec, pnum, lock, stat_collector):
                 continue
+    stat_collector.stop_timer()
+    # pickle stat collector
+    path = pickle_path / f"{pnum}.pickle"
+    print(f"pickling into {path}")
+    with open(path, 'wb') as f:
+        pickle.dump(stat_collector, f)
 
-time_limit = 600  # time limit for searching
+time_limit = args.time_limit  # time limit for searching
 
-if not os.path.isdir("./res_%04d"%time_limit):
-    os.makedirs("./res_%04d"%time_limit)
+if not os.path.isdir(args.output + "/res_%04d"%time_limit):
+    os.makedirs(args.output + "/res_%04d"%time_limit)
 
 # for graph reduction and local search
-# api = reducelib()
+if args.local_search or args.reduction:
+    api = reducelib()
+
+if args.self_loops:
+    import scipy
 
 for id in range(len(val_mat_names)):
 
@@ -309,11 +369,21 @@ for id in range(len(val_mat_names)):
     best_IS_vec = []
 
     lock = Lock()
-
+    stat_collector = statistics.GraphResultCollector(val_mat_names[id])
+    stat_collector.start_timer()
     mat_contents = sio.loadmat(data_path + '/' + val_mat_names[id])
     adj_0 = mat_contents['adj']
-    # yy = mat_contents['indset_label']
-    # opt_num = np.sum(yy[:, 0])
+    if args.self_loops:
+        identity = scipy.sparse.identity(adj_0.shape[0], dtype=adj_0.dtype, format=adj_0.format)
+        adj_0 = adj_0 + identity
+
+    labels_given = False
+    if 'indset_label' in mat_contents.keys():
+        yy = mat_contents['indset_label']
+        opt_num = np.sum(yy[:,0])
+        labels_given = True
+        print("Labels were given, terminating if optimal MIS found", file=sys.stderr)
+
     # edges_0 = sp.find(adj_0) # for isis version 1
     edges_0 = findNodeEdges(adj_0)
     nn = adj_0.shape[0]
@@ -326,7 +396,8 @@ for id in range(len(val_mat_names)):
 
     start_time = time.time()
 
-    processes = [mp.Process(target=MPSearch, args=(pnum, lock)) for pnum in range(16)]
+    pickle_path = Path(tempfile.mkdtemp())
+    processes = [mp.Process(target=MPSearch, args=(pnum, lock, stat_collector, pickle_path, labels_given)) for pnum in range(args.num_threads)]
 
     # Run processes
     for p in processes:
@@ -336,7 +407,24 @@ for id in range(len(val_mat_names)):
     for p in processes:
         p.join()
 
+    rclist = []
+
+    # todo fetch results here
+    for f_path in pickle_path.rglob("*.pickle"):
+        print(f"Reading file {str(f_path)} into result list.")
+        with open(f_path, 'rb') as f:
+            result = pickle.load(f)
+        rclist.append(result)
+
+    if len(rclist) != args.num_threads:
+        print(f"Read {len(rclist)} results, but we had {args.num_threads} threads. Something is off!")
+
+    shutil.rmtree(pickle_path)
+    statistics.collector.collectors.append(reduce(lambda x, y: x + y, rclist))
+
     print(time.time() - start_time)
 
     # sio.savemat('result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/res_tbf_mp_e_satlib_%04d/%s' % (time_limit, val_mat_names[id]),
     #                 {'er_graph': adj_0, 'nIS_vec': best_IS_vec})
+
+statistics.collector.finalize(args.output + "/results.json")
diff --git a/gcn/inits.py b/gcn/inits.py
index 4a2f1ae..aff8e64 100644
--- a/gcn/inits.py
+++ b/gcn/inits.py
@@ -1,4 +1,6 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
+
 import numpy as np
 
 
@@ -24,4 +26,4 @@ def zeros(shape, name=None):
 def ones(shape, name=None):
     """All ones."""
     initial = tf.ones(shape, dtype=tf.float32)
-    return tf.Variable(initial, name=name)
\ No newline at end of file
+    return tf.Variable(initial, name=name)
diff --git a/gcn/layers.py b/gcn/layers.py
index 0035cbb..14ac2c2 100644
--- a/gcn/layers.py
+++ b/gcn/layers.py
@@ -1,7 +1,8 @@
 from inits import *
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 
-flags = tf.app.flags
+flags = tf.flags
 FLAGS = flags.FLAGS
 
 # global unique layer ID dictionary for layer name assignment
diff --git a/gcn/metrics.py b/gcn/metrics.py
index cb82706..0a7e227 100644
--- a/gcn/metrics.py
+++ b/gcn/metrics.py
@@ -1,4 +1,6 @@
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
+
 
 def my_softmax_cross_entropy(preds, labels):
     """Softmax cross-entropy loss with masking."""
diff --git a/gcn/models.py b/gcn/models.py
index b8e9240..2beb2de 100644
--- a/gcn/models.py
+++ b/gcn/models.py
@@ -2,7 +2,7 @@ from layers import *
 from metrics import *
 from layers import _LAYER_UIDS
 
-flags = tf.app.flags
+flags = tf.flags
 FLAGS = flags.FLAGS
 
 def lrelu(x):
@@ -224,4 +224,4 @@ class GCN_DEEP_DIVER(Model):
                                             logging=self.logging))
 
     def predict(self):
-        return tf.nn.softmax(self.outputs)
\ No newline at end of file
+        return tf.nn.softmax(self.outputs)
diff --git a/statistics.py b/statistics.py
new file mode 100644
index 0000000..c34f928
--- /dev/null
+++ b/statistics.py
@@ -0,0 +1,111 @@
+import json
+import time
+import numpy as np
+import os
+
+class GraphResultCollector():
+    def __init__(self, graph_name):
+        self.best_mis = None
+        self.best_mis_time = None
+        self.best_mis_size = 0
+        self.total_solutions = 0
+        self.results = {}
+        self.graph_name = graph_name.replace(".mat", "") # do not use splitext here, due to recursive calls!
+
+    def start_timer(self):
+        self.start_time = time.monotonic()
+
+    def start_process_timer(self):
+        self.process_start_time = time.process_time()
+
+    def collect_result(self, mis):
+        mis_len = np.ravel(mis).shape[0] 
+        if mis_len > self.best_mis_size:
+            self.best_mis_time = time.monotonic() - self.start_time
+            self.best_mis_process_time = time.process_time() - self.process_start_time
+            self.best_mis = mis
+            self.best_mis_size = mis_len
+
+    def add_iteration(self):
+        self.total_solutions += 1
+
+    def stop_timer(self):
+        self.total_time = time.monotonic() - self.start_time
+
+    def finalize(self):
+        if self.best_mis is not None:
+            return {
+                "found_mis": True,
+                "vertices": self.best_mis_size,
+                "solution_time": self.best_mis_time,
+                "mis": np.ravel(self.best_mis).tolist(),
+                "total_solutions": self.total_solutions,
+                "total_time": self.total_time,
+                "solution_process_time": self.best_mis_process_time
+            }
+        else:
+            return {
+                "found_mis": False,
+                "total_solutions": self.total_solutions,
+                "total_time": self.total_time
+            }
+
+    def __add__(self, gcol):
+        if self.graph_name != gcol.graph_name:
+            raise Exception("Trying to merge two graph collectors of different graphs")
+
+        res = GraphResultCollector(self.graph_name)
+        res.total_solutions = self.total_solutions + gcol.total_solutions
+        res.total_time = max(self.total_time, gcol.total_time)
+           
+        if self.best_mis is None and gcol.best_mis is not None:
+            self.best_mis_size = -1
+       
+        if gcol.best_mis is None and self.best_mis is not None:
+            gcol.best_mis_size = -1
+
+        if gcol.best_mis is None and self.best_mis is None:
+            return res
+
+        if self.best_mis_size > gcol.best_mis_size:
+            res.best_mis = self.best_mis
+            res.best_mis_size = self.best_mis_size
+            res.best_mis_time = self.best_mis_time
+            res.best_mis_process_time = self.best_mis_process_time
+        else:
+            res.best_mis = gcol.best_mis
+            res.best_mis_size = gcol.best_mis_size
+            res.best_mis_time = gcol.best_mis_time
+            res.best_mis_process_time = gcol.best_mis_process_time
+
+        return res
+            
+
+class ResultCollector():
+    def __init__(self):
+        self.collectors = []
+        self.current_collector = None
+
+    def new_collector(self, graph_name):
+        if self.collectors:
+            self.current_collector.stop_timer()
+
+        g = GraphResultCollector(graph_name)
+        self.collectors += [g]
+        self.current_collector = g
+
+        return g
+    def finalize(self, out_path):
+        if self.current_collector:
+            self.current_collector.stop_timer()
+        results = {}
+        for g in self.collectors:
+            results[g.graph_name] = g.finalize()
+        self.dump(results, out_path)
+    def dump(self, results, out_path):
+        with open(out_path, 'w', encoding='utf-8') as f:
+            json.dump(results, f, ensure_ascii=False, sort_keys = True, indent=4)
+
+
+collector = ResultCollector()
+
diff --git a/train.py b/train.py
index 7156dbf..5030cfd 100644
--- a/train.py
+++ b/train.py
@@ -3,6 +3,7 @@ from __future__ import print_function
 
 import sys
 import os
+import random
 sys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )
 
 import time
@@ -11,28 +12,58 @@ import numpy as np
 import scipy.sparse as sp
 from copy import deepcopy
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+tf.disable_v2_behavior()
 from utils import *
 from models import GCN_DEEP_DIVER
+from pathlib import Path
+
+### Begin argument parsing
+import argparse
+
+parser = argparse.ArgumentParser(description="Intel-based tree search.")
+parser.add_argument("input", type=str, action="store", help="Directory containing input graphs (to be solved/trained on).")
+parser.add_argument("output", type=str, action="store",  help="Folder in which the output (e.g. json containg statistics and solution will be stored, or trained weights)")
+
+parser.add_argument("--cuda_device", type=int, nargs="*", action="store", default=0, help="Which cuda device should be used")
+parser.add_argument("--self_loops", action="store_true", default=False, help="Enable self loops addition (in input data) for GCN-based model.")
+parser.add_argument("--model_prob_maps", type=int, action="store", default=32, help="Number of probability maps the model was/should be trained for.")
+parser.add_argument("--lr", type=float, action="store", default=0.001, help="Learning rate (for training)")
+parser.add_argument("--epochs", type=int, action="store", default=20, help="Number of epochs to train for (notion changed compared to original Intel version, see paper for details)")
+parser.add_argument("--pretrained_weights", type=str, action="store", help="Pre-trained weights to continue training on")
+
+args = parser.parse_args()
+
 
 N_bd = 32
 
 # Settings
-flags = tf.app.flags
+flags = tf.flags
 FLAGS = flags.FLAGS
 flags.DEFINE_string('model', 'gcn_cheby', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'
-flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')
-flags.DEFINE_integer('epochs', 201, 'Number of epochs to train.')
+flags.DEFINE_float('learning_rate', args.lr, 'Initial learning rate.')
+flags.DEFINE_integer('epochs', args.epochs, 'Number of epochs to train.')
 flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')
-flags.DEFINE_integer('diver_num', 32, 'Number of outputs.')
+flags.DEFINE_integer('diver_num', args.model_prob_maps, 'Number of outputs.')
 flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probaNUmbility).')
 flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')
 flags.DEFINE_integer('early_stopping', 1000, 'Tolerance for early stopping (# of epochs).')
 flags.DEFINE_integer('max_degree', 1, 'Maximum Chebyshev polynomial degree.')
 flags.DEFINE_integer('num_layer', 20, 'number of layers.')
 
+# we need to define our argparse argument here aswell, otherwise tf.flags throws an exception
+flags.DEFINE_string("cuda_device", "", "")
+flags.DEFINE_boolean("self_loops", False, "")
+flags.DEFINE_boolean("reduction", False, "")
+flags.DEFINE_boolean("local_search", False, "")
+flags.DEFINE_string("model_prob_maps", "", "")
+flags.DEFINE_string("lr", "", "")
+flags.DEFINE_string("pretrained_weights", "", "")
+
 # Load data
-data_path = "./data/CBS_Graph"
+data_path = args.input
+if not Path(data_path).exists():
+    raise ValueError(f"Input directory {data_path} does not exists")
 train_mat_names = os.listdir(data_path)
 
 # Some preprocessing
@@ -54,7 +85,7 @@ placeholders = {
 model = model_func(placeholders, input_dim=N_bd, logging=True)
 
 # use gpu 0
-os.environ['CUDA_VISIBLE_DEVICES']=str(0)
+os.environ['CUDA_VISIBLE_DEVICES']=str(args.cuda_device)
 
 # Initialize session
 config = tf.ConfigProto()
@@ -72,30 +103,40 @@ def evaluate(features, support, labels, mask, placeholders):
 saver=tf.train.Saver(max_to_keep=1000)
 sess.run(tf.global_variables_initializer())
 
-ckpt=tf.train.get_checkpoint_state("result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32")
-if ckpt:
+ckpt = None
+if args.pretrained_weights:
+    ckpt=tf.train.get_checkpoint_state(args.pretrained_weights)
     print('loaded '+ckpt.model_checkpoint_path)
     saver.restore(sess,ckpt.model_checkpoint_path)
 
 # cost_val = []
 
-all_loss = np.zeros(2000, dtype=float)
-all_acc = np.zeros(2000, dtype=float)
+all_loss = np.zeros(len(train_mat_names), dtype=float)
+all_acc = np.zeros(len(train_mat_names), dtype=float)
+
+if args.self_loops:
+    import scipy
 
 # Train model
 for epoch in range(FLAGS.epochs):
-    if os.path.isdir("result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/%04d"%epoch):
+    if os.path.isdir(args.output + "/%04d"%epoch):
         continue
     ct = 0
-    os.makedirs("result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/%04d" % epoch)
+    os.makedirs(args.output+"/%04d" % epoch)
     # for id in np.random.permutation(len(train_mat_names)):
-    for idd in range(2000):
-        id = np.random.randint(38000)
+    ids = list(range(len(train_mat_names)))
+    random.shuffle(ids)
+    for id in ids:
         ct = ct + 1
         t = time.time()
         # load data
         mat_contents = sio.loadmat(data_path+'/'+train_mat_names[id])
         adj = mat_contents['adj']
+
+        if args.self_loops:
+            identity = scipy.sparse.identity(adj.shape[0], dtype=adj.dtype, format=adj.format)
+            adj = adj + identity
+
         yy = mat_contents['indset_label']
         nn, nr = yy.shape # number of nodes & results
         # y_train = yy[:,np.random.randint(0,nr)]
@@ -141,11 +182,11 @@ for epoch in range(FLAGS.epochs):
               "train_acc=", "{:.5f}".format(np.mean(all_acc[np.where(all_acc)])), "time=", "{:.5f}".format(time.time() - t))
 
 
-    target=open("result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/%04d/score.txt"%epoch,'w')
+    target=open(args.output+"/%04d/score.txt"%epoch,'w')
     target.write("%f\n%f\n"%(np.mean(all_loss[np.where(all_loss)]),np.mean(all_acc[np.where(all_acc)])))
     target.close()
 
-    saver.save(sess,"result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/model.ckpt")
-    saver.save(sess,"result_IS4SAT_deep_ld32_c32_l20_cheb1_diver32_res32/%04d/model.ckpt"%epoch)
+    saver.save(sess,args.output + "/model.ckpt")
+    saver.save(sess,args.output + "/%04d/model.ckpt"%epoch)
 
-print("Optimization Finished!")
\ No newline at end of file
+print("Optimization Finished!")
